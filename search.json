[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Shivani Chowdhry",
    "section": "",
    "text": "This is my personal website which is built mainly to showcase the coding assignments I have worked on in two courses of my Master’s degree in Social Data Analytics and Research at University of Texas at Dallas.\n\nplot(mtcars, cex=0.9, pch=16, col=\"firebrick\")"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Research",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "assignment1_6302.html",
    "href": "assignment1_6302.html",
    "title": "Shivani Chowdhry",
    "section": "",
    "text": "## Anscombe (1973) Quartlet\n\ndata(anscombe)  # Load Anscombe's data\nView(anscombe) # View the data\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n## Simple version\nplot(anscombe$x1,anscombe$y1)\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n# Create four model objects\nlm1 <- lm(y1 ~ x1, data=anscombe)\nsummary(lm1)\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\nlm2 <- lm(y2 ~ x2, data=anscombe)\nsummary(lm2)\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\nlm3 <- lm(y3 ~ x3, data=anscombe)\nsummary(lm3)\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\nlm4 <- lm(y4 ~ x4, data=anscombe)\nsummary(lm4)\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\nplot(anscombe$x1,anscombe$y1)\nabline(coefficients(lm1))\n\n\n\nplot(anscombe$x2,anscombe$y2)\nabline(coefficients(lm2))\n\n\n\nplot(anscombe$x3,anscombe$y3)\nabline(coefficients(lm3))\n\n\n\nplot(anscombe$x4,anscombe$y4)\nabline(coefficients(lm4))\n\n\n\n## Fancy version (per help file)\n\nff <- y ~ x\nmods <- setNames(as.list(1:4), paste0(\"lm\", 1:4))\n\n# Plot using for loop\nfor(i in 1:4) {\n  ff[2:3] <- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  ## or   ff[[2]] <- as.name(paste0(\"y\", i))\n  ##      ff[[3]] <- as.name(paste0(\"x\", i))\n  mods[[i]] <- lmi <- lm(ff, data = anscombe)\n  print(anova(lmi))\n}\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(>F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(>F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(>F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y4\n          Df Sum Sq Mean Sq F value   Pr(>F)   \nx4         1 27.490 27.4900  18.003 0.002165 **\nResiduals  9 13.742  1.5269                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsapply(mods, coef)  # Note the use of this function\n\n                  lm1      lm2       lm3       lm4\n(Intercept) 3.0000909 3.000909 3.0024545 3.0017273\nx1          0.5000909 0.500000 0.4997273 0.4999091\n\nlapply(mods, function(fm) coef(summary(fm)))\n\n$lm1\n             Estimate Std. Error  t value    Pr(>|t|)\n(Intercept) 3.0000909  1.1247468 2.667348 0.025734051\nx1          0.5000909  0.1179055 4.241455 0.002169629\n\n$lm2\n            Estimate Std. Error  t value    Pr(>|t|)\n(Intercept) 3.000909  1.1253024 2.666758 0.025758941\nx2          0.500000  0.1179637 4.238590 0.002178816\n\n$lm3\n             Estimate Std. Error  t value    Pr(>|t|)\n(Intercept) 3.0024545  1.1244812 2.670080 0.025619109\nx3          0.4997273  0.1178777 4.239372 0.002176305\n\n$lm4\n             Estimate Std. Error  t value    Pr(>|t|)\n(Intercept) 3.0017273  1.1239211 2.670763 0.025590425\nx4          0.4999091  0.1178189 4.243028 0.002164602\n\n# Preparing for the plots\nop <- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))\n\n# Plot charts using for loop\nfor(i in 1:4) {\n  ff[2:3] <- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"red\", pch = 21, bg = \"orange\", cex = 1.2,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"blue\")\n}\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5)\n\n\n\npar(op)"
  },
  {
    "objectID": "assignment1_6356.html",
    "href": "assignment1_6356.html",
    "title": "Shivani Chowdhry",
    "section": "",
    "text": "Q1. Try Anscombe’s examples (anscombe01.R on Teams)\n\n## Anscombe (1973) Quartlet\n\ndata(anscombe)  # Load Anscombe's data\nView(anscombe) # View the data\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n## Simple version\nplot(anscombe$x1,anscombe$y1)\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n# Create four model objects\nlm1 <- lm(y1 ~ x1, data=anscombe)\nsummary(lm1)\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\nlm2 <- lm(y2 ~ x2, data=anscombe)\nsummary(lm2)\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\nlm3 <- lm(y3 ~ x3, data=anscombe)\nsummary(lm3)\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\nlm4 <- lm(y4 ~ x4, data=anscombe)\nsummary(lm4)\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\nplot(anscombe$x1,anscombe$y1)\nabline(coefficients(lm1))\n\n\n\nplot(anscombe$x2,anscombe$y2)\nabline(coefficients(lm2))\n\n\n\nplot(anscombe$x3,anscombe$y3)\nabline(coefficients(lm3))\n\n\n\nplot(anscombe$x4,anscombe$y4)\nabline(coefficients(lm4))\n\n\n\n## Fancy version (per help file)\n\nff <- y ~ x\nmods <- setNames(as.list(1:4), paste0(\"lm\", 1:4))\n\n# Plot using for loop\nfor(i in 1:4) {\n  ff[2:3] <- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  ## or   ff[[2]] <- as.name(paste0(\"y\", i))\n  ##      ff[[3]] <- as.name(paste0(\"x\", i))\n  mods[[i]] <- lmi <- lm(ff, data = anscombe)\n  print(anova(lmi))\n}\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(>F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(>F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(>F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y4\n          Df Sum Sq Mean Sq F value   Pr(>F)   \nx4         1 27.490 27.4900  18.003 0.002165 **\nResiduals  9 13.742  1.5269                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsapply(mods, coef)  # Note the use of this function\n\n                  lm1      lm2       lm3       lm4\n(Intercept) 3.0000909 3.000909 3.0024545 3.0017273\nx1          0.5000909 0.500000 0.4997273 0.4999091\n\nlapply(mods, function(fm) coef(summary(fm)))\n\n$lm1\n             Estimate Std. Error  t value    Pr(>|t|)\n(Intercept) 3.0000909  1.1247468 2.667348 0.025734051\nx1          0.5000909  0.1179055 4.241455 0.002169629\n\n$lm2\n            Estimate Std. Error  t value    Pr(>|t|)\n(Intercept) 3.000909  1.1253024 2.666758 0.025758941\nx2          0.500000  0.1179637 4.238590 0.002178816\n\n$lm3\n             Estimate Std. Error  t value    Pr(>|t|)\n(Intercept) 3.0024545  1.1244812 2.670080 0.025619109\nx3          0.4997273  0.1178777 4.239372 0.002176305\n\n$lm4\n             Estimate Std. Error  t value    Pr(>|t|)\n(Intercept) 3.0017273  1.1239211 2.670763 0.025590425\nx4          0.4999091  0.1178189 4.243028 0.002164602\n\n# Preparing for the plots\nop <- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))\n\n# Plot charts using for loop\nfor(i in 1:4) {\n  ff[2:3] <- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"red\", pch = 21, bg = \"orange\", cex = 1.2,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"blue\")\n}\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5)\n\n\n\npar(op)\n\nQ2. Google “generative art”. Cite some examples.\nGenerative art is a form of art in which an artist uses algorithms, mathematical equations, or computer programs to generate artwork autonomously or along with the artist’s creative input. It often involves the use of randomness, automation, and computational processes to produce unique and unpredictable visual or audio compositions. Generative art blurs the lines between human creativity and machine-generated content, resulting in a wide range of aesthetic expressions.\nHere are some examples of generative art and the artists associated with them:\n\nFractal Art:\n\nMandelbrot Set: The Mandelbrot Set is a famous example of generative art, consisting of intricate and self-replicating fractal patterns. Benoît B. Mandelbrot is the mathematician behind this concept.\n\nAlgorithmic Art:\n\nVera Molnár: She is known for her algorithmic art, particularly her use of systematic processes and rules to create geometric and abstract compositions.\n\nComputer-Generated Landscapes:\n\nKen Musgrave: Musgrave’s Terragen software creates stunning computer-generated landscapes, complete with mountains, rivers, and skies. These landscapes are often used in movies and video games.\n\n\n\nQ3. Run Fall.R (on Teams). Give your own colors (e.g. Spring).\ninstall.packages(\"gsubfn\")\ninstall.packages(\"tidyverse\")\nlibrary(gsubfn)\nlibrary(tidyverse)\n\n# Define elements in plant art\n# Each image corresponds to a different axiom, rules, angle and depth\n\n# Leaf of Fall\n\naxiom=\"X\"\nrules=list(\"X\"=\"F-[[X]+X]+F[+FX]-X\", \"F\"=\"FF\")\nangle=22.5\ndepth=6\n\n\nfor (i in 1:depth) axiom=gsubfn(\".\", rules, axiom)\n\nactions=str_extract_all(axiom, \"\\\\d*\\\\+|\\\\d*\\\\-|F|L|R|\\\\[|\\\\]|\\\\|\") %>% unlist\n\nstatus=data.frame(x=numeric(0), y=numeric(0), alfa=numeric(0))\npoints=data.frame(x1 = 0, y1 = 0, x2 = NA, y2 = NA, alfa=90, depth=1)\n\n\n# Generating data\n# Note: may take a minute or two\n\nfor (action in actions)\n{\n  if (action==\"F\")\n  {\n    x=points[1, \"x1\"]+cos(points[1, \"alfa\"]*(pi/180))\n    y=points[1, \"y1\"]+sin(points[1, \"alfa\"]*(pi/180))\n    points[1,\"x2\"]=x\n    points[1,\"y2\"]=y\n    data.frame(x1 = x, y1 = y, x2 = NA, y2 = NA,\n               alfa=points[1, \"alfa\"],\n               depth=points[1,\"depth\"]) %>% rbind(points)->points\n  }\n  if (action %in% c(\"+\", \"-\")){\n    alfa=points[1, \"alfa\"]\n    points[1, \"alfa\"]=eval(parse(text=paste0(\"alfa\",action, angle)))\n  }\n  if(action==\"[\"){\n    data.frame(x=points[1, \"x1\"], y=points[1, \"y1\"], alfa=points[1, \"alfa\"]) %>%\n      rbind(status) -> status\n    points[1, \"depth\"]=points[1, \"depth\"]+1\n  }\n\n  if(action==\"]\"){\n    depth=points[1, \"depth\"]\n    points[-1,]->points\n    data.frame(x1=status[1, \"x\"], y1=status[1, \"y\"], x2=NA, y2=NA,\n               alfa=status[1, \"alfa\"],\n               depth=depth-1) %>%\n      rbind(points) -> points\n    status[-1,]->status\n  }\n}\n\nggplot() +\n  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2),\n               lineend = \"round\",\n               color=\"forestgreen\", # Set your own Fall color? #Google this document for help w color names in R: http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf\n               data=na.omit(points)) +\n  coord_fixed(ratio = 1) +\n  theme_void() # No grid nor axes"
  },
  {
    "objectID": "assignment2_6302.html",
    "href": "assignment2_6302.html",
    "title": "Google Trends Data Analysis: Using Google Trends website vs gtrendsR package in R",
    "section": "",
    "text": "Google Trends Website to download and visualize time trends with data:\n\nUsing whatever search filters user provides including the search terms or keywords (in this case, I have provided 3 terms, ‘Biden’, ‘Trump’, and ‘Election’), and the time duration and geographical location filters, Google Trends allows us to explore the popularity of these specified terms over time. Google Trends’ interest over time plot for my search keywords looks like this:\n\n\nUsing gtrendsR package in R to pull google trends data and visualize it:\n\ninstall.packages(\"gtrendsR\")\nlibrary(gtrendsR)\nlibrary(ggplot2)\n\nBidenTrumpElection = gtrends(c(\"Trump\",\"Biden\",\"election\"), time = \"all\")\npar(family=\"Georgia\")\nplot(BidenTrumpElection)\n\nGtrendsR package provides a convenient way to retrieve and display Google Trends data in R. It can also be used to retrieve Google Trends data for a specific keyword and geographic region and time period, and allows the use of plot function to visualize the trends over time. One can also use ggplot2 package to create more complex visualizations of this data, for example, I have created below a time-series plot of this data to show time trends of Google searches for these keywords:\n# Explore and preprocess data for time-series plot\ntime_series_data &lt;- BidenTrumpElection$interest_over_time\nlibrary(ggplot2)\n\n# Create a list of colors for keywords\nkeyword_colors &lt;- c(\"blue\", \"green\", \"red\")  # Add more colors if needed\n\n# Create the time-series plot\nggplot(data = time_series_data, aes(x = date)) +\n  geom_line(aes(y = hits, color = keyword)) +\n  scale_color_manual(values = keyword_colors) +\n  xlab(\"Date\") +\n  ylab(\"Search Interest\") +\n  labs(title = \"Google Trends Search Interest Over Time\") +\n  theme_minimal()"
  },
  {
    "objectID": "assignment3_6302.html",
    "href": "assignment3_6302.html",
    "title": "Text Analysis using Quanteda in R",
    "section": "",
    "text": "Latent Semantic Analysis (LSA) is a popular, dimensionality-reduction technique in natural language processing that is used to discover the hidden semantic structure in a large collection of documents. The goal of LSA is to capture the latent (hidden) relationships between words and documents, allowing for a more nuanced understanding of the semantic structure of the text. LSA has been used in various text analysis tasks, such as document clustering, information retrieval, and topic modeling. It is particularly useful when dealing with large and high-dimensional text datasets where the noise or redundancy in the data can be reduced through dimensionality reduction.\nBelow is an example in which this technique is used on Twitter data on Biden-Xi summit held in 2021:\n\n# Sample program for using quanteda for text modeling and analysis\n# Documentation: vignette(\"quickstart\", package = \"quanteda\")\n# Website: https://quanteda.io/\n\n# install.packages(c(\"readr\",quanteda\", \"quanteda.textmodels\", \"quanteda.textplots\",\"quanteda.textstats\",\"tidyverse\"))\nlibrary(quanteda)\n\nWarning: package 'quanteda' was built under R version 4.2.3\n\n\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"ndiMatrix\" of class \"replValueSp\"; definition not updated\n\n\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"pcorMatrix\" of class \"replValueSp\"; definition not updated\n\n\nPackage version: 3.3.1\nUnicode version: 13.0\nICU version: 69.1\n\n\nParallel computing: 8 of 8 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\n\nWarning: package 'quanteda.textmodels' was built under R version 4.2.3\n\nlibrary(quanteda.textplots)\n\nWarning: package 'quanteda.textplots' was built under R version 4.2.3\n\nlibrary(readr)\n\nWarning: package 'readr' was built under R version 4.2.3\n\nlibrary(ggplot2)\n\n# Twitter data about President Biden and Xi summit in Novemeber 2021\n# Do some background search/study on the event\n# \nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\")\n\nRows: 14520 Columns: 90\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (50): screen_name, text, source, reply_to_screen_name, hashtags, symbol...\ndbl  (26): user_id, status_id, display_text_width, reply_to_status_id, reply...\nlgl  (10): is_quote, is_retweet, quote_count, reply_count, ext_media_type, q...\ndttm  (4): created_at, quoted_created_at, retweet_created_at, account_create...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nView(summit)\n\nWe then create a new object sum_twt containing the text of the tweets and tokenize them using the tokens function from the quanteda package. Finally, we create a document-feature matrix (dfm) using the dfm function.\n\n# Create a new object containing the tweets data\nsum_twt = summit$text\ntoks = tokens(sum_twt)\nsumtwtdfm &lt;- dfm(toks)\n\nNow, sumtwtdfm should be a document-feature matrix that can be used for further analysis, such as sentiment analysis, word frequency analysis, or topic modeling. We will be performing LSA in the code below:\n\n# Latent Semantic Analysis\nsum_lsa &lt;- textmodel_lsa(sumtwtdfm)\nsummary(sum_lsa)\n\n                Length    Class     Mode   \nsk                     10 -none-    numeric\ndocs               145200 -none-    numeric\nfeatures           160960 -none-    numeric\nmatrix_low_rank 233713920 -none-    numeric\ndata            233713920 dgCMatrix S4     \n\n## Tokenizing the tweet text again, this time removing punctuation, and then creating a new document-feature matrix\ntweet_dfm &lt;- tokens(sum_twt, remove_punct = TRUE) %&gt;%\n  dfm()\nhead(tweet_dfm)\n\nDocument-feature matrix of: 6 documents, 16,029 features (99.89% sparse) and 0 docvars.\n       features\ndocs    breaking news us president biden amp communist china leader xi\n  text1        1    1  1         1     1   1         1     2      1  1\n  text2        0    0  0         0     0   0         0     0      0  0\n  text3        0    0  0         0     1   0         0     0      0  1\n  text4        0    0  0         1     1   0         0     0      0  1\n  text5        0    0  0         0     1   0         0     0      0  1\n  text6        0    0  0         0     1   0         0     0      0  1\n[ reached max_nfeat ... 16,019 more features ]\n\n## Extracting and analyzing hashtags\ntag_dfm &lt;- dfm_select(tweet_dfm, pattern = \"#*\")\ntoptag &lt;- names(topfeatures(tag_dfm, 50))\nhead(toptag, 10)\n\n [1] \"#china\"          \"#biden\"          \"#xijinping\"      \"#joebiden\"      \n [5] \"#america\"        \"#americans\"      \"#coronavirus\"    \"#fentanyl\"      \n [9] \"#xi\"             \"#uyghurgenocide\"\n\n\nSo, the top 10 hashtags used in the tweets are:\n\"#china\"          \"#biden\"          \"#xijinping\"      \"#joebiden\"       \"#america\" \"#americans\"      \"#coronavirus\"    \"#fentanyl\"       \"#xi\"        \"#uyghurgenocide\"\nNow, we are using the quanteda.textplots package to visualize a network plot of co-occurrence relationships among hashtags in the tweet data. The parameters like min_freq, edge_alpha, and edge_size control the appearance of the network plot.\n\n## Visualizing a network plot of relationships among hashtags in tweets\nlibrary(\"quanteda.textplots\")\ntag_fcm &lt;- fcm(tag_dfm)\nhead(tag_fcm)\n\nFeature co-occurrence matrix of: 6 by 685 features.\n               features\nfeatures        #breaking #breakingnews #biden #china #usa #pray4america\n  #breaking             0             4      4      5    5             0\n  #breakingnews         0             0      4      5    4             0\n  #biden                0             0      0    415   44             0\n  #china                0             0      0      8   76             0\n  #usa                  0             0      0      0    6             0\n  #pray4america         0             0      0      0    0             0\n               features\nfeatures        #joebiden #xijinping #america #americans\n  #breaking             0          0        0          0\n  #breakingnews         0          0        0          0\n  #biden              299        366      301        295\n  #china              339        433      308        295\n  #usa                 12         14        0          0\n  #pray4america         0          0        0          0\n[ reached max_nfeat ... 675 more features ]\n\ntopgat_fcm &lt;- fcm_select(tag_fcm, pattern = toptag)\ntextplot_network(topgat_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 5)\n\n\n\n\nThe above plot shows that the most important hashtags that frequently occur together are coronavirus, fentanyl, americans, and the two heads of state who attended the summit. Some hashtags are forming central nodes connecting different clusters, for example, hashtag biden is associated with hashtags of human rights, Uyghurs and Tibetians, whereas hashtag China is associated with hashtags discussing relationships between US, China and Taiwan.\nNext. we are working with user mentions (Twitter handles) in the tweet data. First we select terms (features) from tweet_dfm that match the pattern “@*” (user mentions), then identifiy the top 50 user mentions using the topfeatures function and store their names in the topuser object. The head function is used to display the first 20 user mentions. Then we are creating a feature co-occurence matrix based on document of user mentions, and displaying the first 20 rows. Then the code selects only the co-occurrence relationships of the top user mentions identified earlier from the entire co-occurrence matrix and uses textplot_network to create a network plot of the co-occurrence relationships among the top user mentions.\n\nuser_dfm &lt;- dfm_select(tweet_dfm, pattern = \"@*\")\ntopuser &lt;- names(topfeatures(user_dfm, 50))\nhead(topuser, 20)\n\n [1] \"@potus\"           \"@joebiden\"        \"@politico\"        \"@eneskanter\"     \n [5] \"@jendeben\"        \"@nwadhams\"        \"@nba\"             \"@washwizards\"    \n [9] \"@pelicansnba\"     \"@capitalonearena\" \"@kevinliptakcnn\"  \"@foxbusiness\"    \n[13] \"@morningsmaria\"   \"@scmpnews\"        \"@uyghur_american\" \"@nytimes\"        \n[17] \"@petermartin_pcm\" \"@nahaltoosi\"      \"@phelimkine\"      \"@kaylatausche\"   \n\nuser_fcm &lt;- fcm(user_dfm)\nhead(user_fcm, 20)\n\nFeature co-occurrence matrix of: 20 by 741 features.\n                 features\nfeatures          @youtube @bfmtv @cnn @lauhaim @barackobama @joebiden\n  @youtube               0      0    0        0            0         0\n  @bfmtv                 0      0    1        1            1         1\n  @cnn                   0      0    0        1            1         1\n  @lauhaim               0      0    0        0            1         1\n  @barackobama           0      0    0        0            0         1\n  @joebiden              0      0    0        0            0         3\n  @kamalaharris          0      0    0        0            0         0\n  @hillaryclinton        0      0    0        0            0         0\n  @billclinton           0      0    0        0            0         0\n  @cbsnews               0      0    0        0            0         0\n                 features\nfeatures          @kamalaharris @hillaryclinton @billclinton @cbsnews\n  @youtube                    0               0            0        0\n  @bfmtv                      1               1            1        1\n  @cnn                        1               1            1        1\n  @lauhaim                    1               1            1        1\n  @barackobama                1               1            1        1\n  @joebiden                   1               1            1        1\n  @kamalaharris               0               1            1        1\n  @hillaryclinton             0               0            1        1\n  @billclinton                0               0            0        1\n  @cbsnews                    0               0            0        0\n[ reached max_feat ... 10 more features, reached max_nfeat ... 731 more features ]\n\nuser_fcm &lt;- fcm_select(user_fcm, pattern = topuser)\ntextplot_network(user_fcm, min_freq = 20, edge_color = \"firebrick\", edge_alpha = 0.8, edge_size = 5)\n\n\n\n\nAs the plot shows, the bulk of the Tweet discussion is surrounding the users @capitalonearena, @nba, @eneskanter, @pelicansnba, and @washwizards. Thus, network analysis of users in a Twitter discussion canreveal valuable insights into the structure, dynamics, and patterns of interactions among clusters or communities of users who frequently interact with each other. In other words, it helps identify influential users or hubs who have a large number of connections.\n\n\n\nNow, I’ll walk through some of the quanteda package’s extra features for text analysis and visualization to analyze how frequently particular terms are used in US presidential speeches. The corpus of American presidential speeches since 1949 is subjected to a Keyword-in-Context (KWIC) analysis, and for the visualization, I have created X-ray maps for any mentions of the terms “American”, “People” and “communist” that illustrates the words’s distribution and context in a graphical representtation.\n\nlibrary(quanteda.textstats)\n\nWarning: package 'quanteda.textstats' was built under R version 4.2.3\n\n\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"ndiMatrix\" of class \"replValueSp\"; definition not updated\n\n\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"pcorMatrix\" of class \"replValueSp\"; definition not updated\n\ndata_corpus_inaugural_subset &lt;- \ncorpus_subset(data_corpus_inaugural, Year &gt; 1949)\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"american\") %&gt;%\n  textplot_xray()\n\n\n\n\n\ntextplot_xray(\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"american\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"people\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"communist\")\n)\n\n\n\n\n\ntheme_set(theme_bw())\ng &lt;- textplot_xray(\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"american\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"people\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"communist\")\n)\ng + aes(color = keyword) + \n  scale_color_manual(values = c(\"blue\", \"red\", \"green\")) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "assignment3_6302.html#section",
    "href": "assignment3_6302.html#section",
    "title": "Shivani Chowdhry",
    "section": "",
    "text": "title: “Text Analysis using Quanteda in R” ---\nLatent Semantic Analysis (LSA) is a popular, dimensionality-reduction technique in natural language processing that is used to discover the hidden semantic structure in a large collection of documents. The goal of LSA is to capture the latent (hidden) relationships between words and documents, allowing for a more nuanced understanding of the semantic structure of the text. LSA has been used in various text analysis tasks, such as document clustering, information retrieval, and topic modeling. It is particularly useful when dealing with large and high-dimensional text datasets where the noise or redundancy in the data can be reduced through dimensionality reduction.\nBelow is an example in which this technique is used on Twitter data on Biden-Xi summit held in 2021:\n\n# Sample program for using quanteda for text modeling and analysis\n# Documentation: vignette(\"quickstart\", package = \"quanteda\")\n# Website: https://quanteda.io/\n\n# install.packages(c(\"readr\",quanteda\", \"quanteda.textmodels\", \"quanteda.textplots\",\"quanteda.textstats\",\"tidyverse\"))\nlibrary(quanteda)\n\nWarning: package 'quanteda' was built under R version 4.2.3\n\n\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"ndiMatrix\" of class \"replValueSp\"; definition not updated\n\n\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"pcorMatrix\" of class \"replValueSp\"; definition not updated\n\n\nPackage version: 3.3.1\nUnicode version: 13.0\nICU version: 69.1\n\n\nParallel computing: 8 of 8 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\n\nWarning: package 'quanteda.textmodels' was built under R version 4.2.3\n\nlibrary(quanteda.textplots)\n\nWarning: package 'quanteda.textplots' was built under R version 4.2.3\n\nlibrary(readr)\n\nWarning: package 'readr' was built under R version 4.2.3\n\nlibrary(ggplot2)\n\n# Twitter data about President Biden and Xi summit in Novemeber 2021\n# Do some background search/study on the event\n# \nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\")\n\nRows: 14520 Columns: 90\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (50): screen_name, text, source, reply_to_screen_name, hashtags, symbol...\ndbl  (26): user_id, status_id, display_text_width, reply_to_status_id, reply...\nlgl  (10): is_quote, is_retweet, quote_count, reply_count, ext_media_type, q...\ndttm  (4): created_at, quoted_created_at, retweet_created_at, account_create...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nView(summit)\n\nWe then create a new object sum_twt containing the text of the tweets and tokenize them using the tokens function from the quanteda package. Finally, we create a document-feature matrix (dfm) using the dfm function.\n\n# Create a new object containing the tweets data\nsum_twt = summit$text\ntoks = tokens(sum_twt)\nsumtwtdfm &lt;- dfm(toks)\n\nNow, sumtwtdfm should be a document-feature matrix that can be used for further analysis, such as sentiment analysis, word frequency analysis, or topic modeling. We will be performing LSA in the code below:\n\n# Latent Semantic Analysis\nsum_lsa &lt;- textmodel_lsa(sumtwtdfm)\nsummary(sum_lsa)\n\n                Length    Class     Mode   \nsk                     10 -none-    numeric\ndocs               145200 -none-    numeric\nfeatures           160960 -none-    numeric\nmatrix_low_rank 233713920 -none-    numeric\ndata            233713920 dgCMatrix S4     \n\n## Tokenizing the tweet text again, this time removing punctuation, and then creating a new document-feature matrix\ntweet_dfm &lt;- tokens(sum_twt, remove_punct = TRUE) %&gt;%\n  dfm()\nhead(tweet_dfm)\n\nDocument-feature matrix of: 6 documents, 16,029 features (99.89% sparse) and 0 docvars.\n       features\ndocs    breaking news us president biden amp communist china leader xi\n  text1        1    1  1         1     1   1         1     2      1  1\n  text2        0    0  0         0     0   0         0     0      0  0\n  text3        0    0  0         0     1   0         0     0      0  1\n  text4        0    0  0         1     1   0         0     0      0  1\n  text5        0    0  0         0     1   0         0     0      0  1\n  text6        0    0  0         0     1   0         0     0      0  1\n[ reached max_nfeat ... 16,019 more features ]\n\n## Extracting and analyzing hashtags\ntag_dfm &lt;- dfm_select(tweet_dfm, pattern = \"#*\")\ntoptag &lt;- names(topfeatures(tag_dfm, 50))\nhead(toptag, 10)\n\n [1] \"#china\"          \"#biden\"          \"#xijinping\"      \"#joebiden\"      \n [5] \"#america\"        \"#americans\"      \"#coronavirus\"    \"#fentanyl\"      \n [9] \"#xi\"             \"#uyghurgenocide\"\n\n\nSo, the top 10 hashtags used in the tweets are:\n\"#china\"          \"#biden\"          \"#xijinping\"      \"#joebiden\"       \"#america\" \"#americans\"      \"#coronavirus\"    \"#fentanyl\"       \"#xi\"        \"#uyghurgenocide\"\nNow, we are using the quanteda.textplots package to visualize a network plot of co-occurrence relationships among hashtags in the tweet data. The parameters like min_freq, edge_alpha, and edge_size control the appearance of the network plot.\n\n## Visualizing a network plot of relationships among hashtags in tweets\nlibrary(\"quanteda.textplots\")\ntag_fcm &lt;- fcm(tag_dfm)\nhead(tag_fcm)\n\nFeature co-occurrence matrix of: 6 by 685 features.\n               features\nfeatures        #breaking #breakingnews #biden #china #usa #pray4america\n  #breaking             0             4      4      5    5             0\n  #breakingnews         0             0      4      5    4             0\n  #biden                0             0      0    415   44             0\n  #china                0             0      0      8   76             0\n  #usa                  0             0      0      0    6             0\n  #pray4america         0             0      0      0    0             0\n               features\nfeatures        #joebiden #xijinping #america #americans\n  #breaking             0          0        0          0\n  #breakingnews         0          0        0          0\n  #biden              299        366      301        295\n  #china              339        433      308        295\n  #usa                 12         14        0          0\n  #pray4america         0          0        0          0\n[ reached max_nfeat ... 675 more features ]\n\ntopgat_fcm &lt;- fcm_select(tag_fcm, pattern = toptag)\ntextplot_network(topgat_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 5)\n\n\n\n\nThe above plot shows that the most important hashtags that frequently occur together are coronavirus, fentanyl, americans, and the two heads of state who attended the summit. Some hashtags are forming central nodes connecting different clusters, for example, hashtag biden is associated with hashtags of human rights, Uyghurs and Tibetians, whereas hashtag China is associated with hashtags discussing relationships between US, China and Taiwan.\nNext. we are working with user mentions (Twitter handles) in the tweet data. First we select terms (features) from tweet_dfm that match the pattern “@*” (user mentions), then identifiy the top 50 user mentions using the topfeatures function and store their names in the topuser object. The head function is used to display the first 20 user mentions. Then we are creating a feature co-occurence matrix based on document of user mentions, and displaying the first 20 rows. Then the code selects only the co-occurrence relationships of the top user mentions identified earlier from the entire co-occurrence matrix and uses textplot_network to create a network plot of the co-occurrence relationships among the top user mentions.\n\nuser_dfm &lt;- dfm_select(tweet_dfm, pattern = \"@*\")\ntopuser &lt;- names(topfeatures(user_dfm, 50))\nhead(topuser, 20)\n\n [1] \"@potus\"           \"@joebiden\"        \"@politico\"        \"@eneskanter\"     \n [5] \"@jendeben\"        \"@nwadhams\"        \"@nba\"             \"@washwizards\"    \n [9] \"@pelicansnba\"     \"@capitalonearena\" \"@kevinliptakcnn\"  \"@foxbusiness\"    \n[13] \"@morningsmaria\"   \"@scmpnews\"        \"@uyghur_american\" \"@nytimes\"        \n[17] \"@petermartin_pcm\" \"@nahaltoosi\"      \"@phelimkine\"      \"@kaylatausche\"   \n\nuser_fcm &lt;- fcm(user_dfm)\nhead(user_fcm, 20)\n\nFeature co-occurrence matrix of: 20 by 741 features.\n                 features\nfeatures          @youtube @bfmtv @cnn @lauhaim @barackobama @joebiden\n  @youtube               0      0    0        0            0         0\n  @bfmtv                 0      0    1        1            1         1\n  @cnn                   0      0    0        1            1         1\n  @lauhaim               0      0    0        0            1         1\n  @barackobama           0      0    0        0            0         1\n  @joebiden              0      0    0        0            0         3\n  @kamalaharris          0      0    0        0            0         0\n  @hillaryclinton        0      0    0        0            0         0\n  @billclinton           0      0    0        0            0         0\n  @cbsnews               0      0    0        0            0         0\n                 features\nfeatures          @kamalaharris @hillaryclinton @billclinton @cbsnews\n  @youtube                    0               0            0        0\n  @bfmtv                      1               1            1        1\n  @cnn                        1               1            1        1\n  @lauhaim                    1               1            1        1\n  @barackobama                1               1            1        1\n  @joebiden                   1               1            1        1\n  @kamalaharris               0               1            1        1\n  @hillaryclinton             0               0            1        1\n  @billclinton                0               0            0        1\n  @cbsnews                    0               0            0        0\n[ reached max_feat ... 10 more features, reached max_nfeat ... 731 more features ]\n\nuser_fcm &lt;- fcm_select(user_fcm, pattern = topuser)\ntextplot_network(user_fcm, min_freq = 20, edge_color = \"firebrick\", edge_alpha = 0.8, edge_size = 5)\n\n\n\n\nAs the plot shows, the bulk of the Tweet discussion is surrounding the users @capitalonearena, @nba, @eneskanter, @pelicansnba, and @washwizards. Thus, network analysis of users in a Twitter discussion canreveal valuable insights into the structure, dynamics, and patterns of interactions among clusters or communities of users who frequently interact with each other. In other words, it helps identify influential users or hubs who have a large number of connections."
  },
  {
    "objectID": "assignment3_6302.html#analyzing-presidential-speeches-of-american-presidents-since-1949-using-quanteda-in-r",
    "href": "assignment3_6302.html#analyzing-presidential-speeches-of-american-presidents-since-1949-using-quanteda-in-r",
    "title": "Text Analysis using Quanteda in R",
    "section": "",
    "text": "Now, I’ll walk through some of the quanteda package’s extra features for text analysis and visualization to analyze how frequently particular terms are used in US presidential speeches. The corpus of American presidential speeches since 1949 is subjected to a Keyword-in-Context (KWIC) analysis, and for the visualization, I have created X-ray maps for any mentions of the terms “American”, “People” and “communist” that illustrates the words’s distribution and context in a graphical representtation.\n\nlibrary(quanteda.textstats)\n\nWarning: package 'quanteda.textstats' was built under R version 4.2.3\n\n\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"ndiMatrix\" of class \"replValueSp\"; definition not updated\n\n\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"pcorMatrix\" of class \"replValueSp\"; definition not updated\n\ndata_corpus_inaugural_subset &lt;- \ncorpus_subset(data_corpus_inaugural, Year &gt; 1949)\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"american\") %&gt;%\n  textplot_xray()\n\n\n\n\n\ntextplot_xray(\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"american\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"people\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"communist\")\n)\n\n\n\n\n\ntheme_set(theme_bw())\ng &lt;- textplot_xray(\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"american\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"people\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"communist\")\n)\ng + aes(color = keyword) + \n  scale_color_manual(values = c(\"blue\", \"red\", \"green\")) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "assignment3_6302.html#text-analysis-using-quanteda-in-r",
    "href": "assignment3_6302.html#text-analysis-using-quanteda-in-r",
    "title": "Shivani Chowdhry",
    "section": "",
    "text": "Latent Semantic Analysis (LSA) is a popular, dimensionality-reduction technique in natural language processing that is used to discover the hidden semantic structure in a large collection of documents. The goal of LSA is to capture the latent (hidden) relationships between words and documents, allowing for a more nuanced understanding of the semantic structure of the text. LSA has been used in various text analysis tasks, such as document clustering, information retrieval, and topic modeling. It is particularly useful when dealing with large and high-dimensional text datasets where the noise or redundancy in the data can be reduced through dimensionality reduction.\nBelow is an example in which this technique is used on Twitter data on Biden-Xi summit held in 2021:\n\n# Sample program for using quanteda for text modeling and analysis\n# Documentation: vignette(\"quickstart\", package = \"quanteda\")\n# Website: https://quanteda.io/\n\n# install.packages(c(\"readr\",quanteda\", \"quanteda.textmodels\", \"quanteda.textplots\",\"quanteda.textstats\",\"tidyverse\"))\nlibrary(quanteda)\n\nWarning: package 'quanteda' was built under R version 4.2.3\n\n\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"ndiMatrix\" of class \"replValueSp\"; definition not updated\n\n\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"pcorMatrix\" of class \"replValueSp\"; definition not updated\n\n\nPackage version: 3.3.1\nUnicode version: 13.0\nICU version: 69.1\n\n\nParallel computing: 8 of 8 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\n\nWarning: package 'quanteda.textmodels' was built under R version 4.2.3\n\nlibrary(quanteda.textplots)\n\nWarning: package 'quanteda.textplots' was built under R version 4.2.3\n\nlibrary(readr)\n\nWarning: package 'readr' was built under R version 4.2.3\n\nlibrary(ggplot2)\n\n# Twitter data about President Biden and Xi summit in Novemeber 2021\n# Do some background search/study on the event\n# \nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\")\n\nRows: 14520 Columns: 90\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (50): screen_name, text, source, reply_to_screen_name, hashtags, symbol...\ndbl  (26): user_id, status_id, display_text_width, reply_to_status_id, reply...\nlgl  (10): is_quote, is_retweet, quote_count, reply_count, ext_media_type, q...\ndttm  (4): created_at, quoted_created_at, retweet_created_at, account_create...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nView(summit)\n\nWe then create a new object sum_twt containing the text of the tweets and tokenize them using the tokens function from the quanteda package. Finally, we create a document-feature matrix (dfm) using the dfm function.\n\n# Create a new object containing the tweets data\nsum_twt = summit$text\ntoks = tokens(sum_twt)\nsumtwtdfm &lt;- dfm(toks)\n\nNow, sumtwtdfm should be a document-feature matrix that can be used for further analysis, such as sentiment analysis, word frequency analysis, or topic modeling. We will be performing LSA in the code below:\n\n# Latent Semantic Analysis\nsum_lsa &lt;- textmodel_lsa(sumtwtdfm)\nsummary(sum_lsa)\n\n                Length    Class     Mode   \nsk                     10 -none-    numeric\ndocs               145200 -none-    numeric\nfeatures           160960 -none-    numeric\nmatrix_low_rank 233713920 -none-    numeric\ndata            233713920 dgCMatrix S4     \n\n## Tokenizing the tweet text again, this time removing punctuation, and then creating a new document-feature matrix\ntweet_dfm &lt;- tokens(sum_twt, remove_punct = TRUE) %&gt;%\n  dfm()\nhead(tweet_dfm)\n\nDocument-feature matrix of: 6 documents, 16,029 features (99.89% sparse) and 0 docvars.\n       features\ndocs    breaking news us president biden amp communist china leader xi\n  text1        1    1  1         1     1   1         1     2      1  1\n  text2        0    0  0         0     0   0         0     0      0  0\n  text3        0    0  0         0     1   0         0     0      0  1\n  text4        0    0  0         1     1   0         0     0      0  1\n  text5        0    0  0         0     1   0         0     0      0  1\n  text6        0    0  0         0     1   0         0     0      0  1\n[ reached max_nfeat ... 16,019 more features ]\n\n## Extracting and analyzing hashtags\ntag_dfm &lt;- dfm_select(tweet_dfm, pattern = \"#*\")\ntoptag &lt;- names(topfeatures(tag_dfm, 50))\nhead(toptag, 10)\n\n [1] \"#china\"          \"#biden\"          \"#xijinping\"      \"#joebiden\"      \n [5] \"#america\"        \"#americans\"      \"#coronavirus\"    \"#fentanyl\"      \n [9] \"#xi\"             \"#uyghurgenocide\"\n\n\nSo, the top 10 hashtags used in the tweets are:\n\"#china\"          \"#biden\"          \"#xijinping\"      \"#joebiden\"       \"#america\" \"#americans\"      \"#coronavirus\"    \"#fentanyl\"       \"#xi\"        \"#uyghurgenocide\"\nNow, we are using the quanteda.textplots package to visualize a network plot of co-occurrence relationships among hashtags in the tweet data. The parameters like min_freq, edge_alpha, and edge_size control the appearance of the network plot.\n\n## Visualizing a network plot of relationships among hashtags in tweets\nlibrary(\"quanteda.textplots\")\ntag_fcm &lt;- fcm(tag_dfm)\nhead(tag_fcm)\n\nFeature co-occurrence matrix of: 6 by 685 features.\n               features\nfeatures        #breaking #breakingnews #biden #china #usa #pray4america\n  #breaking             0             4      4      5    5             0\n  #breakingnews         0             0      4      5    4             0\n  #biden                0             0      0    415   44             0\n  #china                0             0      0      8   76             0\n  #usa                  0             0      0      0    6             0\n  #pray4america         0             0      0      0    0             0\n               features\nfeatures        #joebiden #xijinping #america #americans\n  #breaking             0          0        0          0\n  #breakingnews         0          0        0          0\n  #biden              299        366      301        295\n  #china              339        433      308        295\n  #usa                 12         14        0          0\n  #pray4america         0          0        0          0\n[ reached max_nfeat ... 675 more features ]\n\ntopgat_fcm &lt;- fcm_select(tag_fcm, pattern = toptag)\ntextplot_network(topgat_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 5)\n\n\n\n\nThe above plot shows that the most important hashtags that frequently occur together are coronavirus, fentanyl, americans, and the two heads of state who attended the summit. Some hashtags are forming central nodes connecting different clusters, for example, hashtag biden is associated with hashtags of human rights, Uyghurs and Tibetians, whereas hashtag China is associated with hashtags discussing relationships between US, China and Taiwan.\nNext. we are working with user mentions (Twitter handles) in the tweet data. First we select terms (features) from tweet_dfm that match the pattern “@*” (user mentions), then identifiy the top 50 user mentions using the topfeatures function and store their names in the topuser object. The head function is used to display the first 20 user mentions. Then we are creating a feature co-occurence matrix based on document of user mentions, and displaying the first 20 rows. Then the code selects only the co-occurrence relationships of the top user mentions identified earlier from the entire co-occurrence matrix and uses textplot_network to create a network plot of the co-occurrence relationships among the top user mentions.\n\nuser_dfm &lt;- dfm_select(tweet_dfm, pattern = \"@*\")\ntopuser &lt;- names(topfeatures(user_dfm, 50))\nhead(topuser, 20)\n\n [1] \"@potus\"           \"@joebiden\"        \"@politico\"        \"@eneskanter\"     \n [5] \"@jendeben\"        \"@nwadhams\"        \"@nba\"             \"@washwizards\"    \n [9] \"@pelicansnba\"     \"@capitalonearena\" \"@kevinliptakcnn\"  \"@foxbusiness\"    \n[13] \"@morningsmaria\"   \"@scmpnews\"        \"@uyghur_american\" \"@nytimes\"        \n[17] \"@petermartin_pcm\" \"@nahaltoosi\"      \"@phelimkine\"      \"@kaylatausche\"   \n\nuser_fcm &lt;- fcm(user_dfm)\nhead(user_fcm, 20)\n\nFeature co-occurrence matrix of: 20 by 741 features.\n                 features\nfeatures          @youtube @bfmtv @cnn @lauhaim @barackobama @joebiden\n  @youtube               0      0    0        0            0         0\n  @bfmtv                 0      0    1        1            1         1\n  @cnn                   0      0    0        1            1         1\n  @lauhaim               0      0    0        0            1         1\n  @barackobama           0      0    0        0            0         1\n  @joebiden              0      0    0        0            0         3\n  @kamalaharris          0      0    0        0            0         0\n  @hillaryclinton        0      0    0        0            0         0\n  @billclinton           0      0    0        0            0         0\n  @cbsnews               0      0    0        0            0         0\n                 features\nfeatures          @kamalaharris @hillaryclinton @billclinton @cbsnews\n  @youtube                    0               0            0        0\n  @bfmtv                      1               1            1        1\n  @cnn                        1               1            1        1\n  @lauhaim                    1               1            1        1\n  @barackobama                1               1            1        1\n  @joebiden                   1               1            1        1\n  @kamalaharris               0               1            1        1\n  @hillaryclinton             0               0            1        1\n  @billclinton                0               0            0        1\n  @cbsnews                    0               0            0        0\n[ reached max_feat ... 10 more features, reached max_nfeat ... 731 more features ]\n\nuser_fcm &lt;- fcm_select(user_fcm, pattern = topuser)\ntextplot_network(user_fcm, min_freq = 20, edge_color = \"firebrick\", edge_alpha = 0.8, edge_size = 5)\n\n\n\n\nAs the plot shows, the bulk of the Tweet discussion is surrounding the users @capitalonearena, @nba, @eneskanter, @pelicansnba, and @washwizards. Thus, network analysis of users in a Twitter discussion canreveal valuable insights into the structure, dynamics, and patterns of interactions among clusters or communities of users who frequently interact with each other. In other words, it helps identify influential users or hubs who have a large number of connections."
  },
  {
    "objectID": "assignment3_6302.html#using-latent-semantic-analysis-to-analyze-twitter-data",
    "href": "assignment3_6302.html#using-latent-semantic-analysis-to-analyze-twitter-data",
    "title": "Text Analysis using Quanteda in R",
    "section": "",
    "text": "Latent Semantic Analysis (LSA) is a popular, dimensionality-reduction technique in natural language processing that is used to discover the hidden semantic structure in a large collection of documents. The goal of LSA is to capture the latent (hidden) relationships between words and documents, allowing for a more nuanced understanding of the semantic structure of the text. LSA has been used in various text analysis tasks, such as document clustering, information retrieval, and topic modeling. It is particularly useful when dealing with large and high-dimensional text datasets where the noise or redundancy in the data can be reduced through dimensionality reduction.\nBelow is an example in which this technique is used on Twitter data on Biden-Xi summit held in 2021:\n\n# Sample program for using quanteda for text modeling and analysis\n# Documentation: vignette(\"quickstart\", package = \"quanteda\")\n# Website: https://quanteda.io/\n\n# install.packages(c(\"readr\",quanteda\", \"quanteda.textmodels\", \"quanteda.textplots\",\"quanteda.textstats\",\"tidyverse\"))\nlibrary(quanteda)\n\nWarning: package 'quanteda' was built under R version 4.2.3\n\n\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"ndiMatrix\" of class \"replValueSp\"; definition not updated\n\n\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"pcorMatrix\" of class \"replValueSp\"; definition not updated\n\n\nPackage version: 3.3.1\nUnicode version: 13.0\nICU version: 69.1\n\n\nParallel computing: 8 of 8 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\n\nWarning: package 'quanteda.textmodels' was built under R version 4.2.3\n\nlibrary(quanteda.textplots)\n\nWarning: package 'quanteda.textplots' was built under R version 4.2.3\n\nlibrary(readr)\n\nWarning: package 'readr' was built under R version 4.2.3\n\nlibrary(ggplot2)\n\n# Twitter data about President Biden and Xi summit in Novemeber 2021\n# Do some background search/study on the event\n# \nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\")\n\nRows: 14520 Columns: 90\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (50): screen_name, text, source, reply_to_screen_name, hashtags, symbol...\ndbl  (26): user_id, status_id, display_text_width, reply_to_status_id, reply...\nlgl  (10): is_quote, is_retweet, quote_count, reply_count, ext_media_type, q...\ndttm  (4): created_at, quoted_created_at, retweet_created_at, account_create...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nView(summit)\n\nWe then create a new object sum_twt containing the text of the tweets and tokenize them using the tokens function from the quanteda package. Finally, we create a document-feature matrix (dfm) using the dfm function.\n\n# Create a new object containing the tweets data\nsum_twt = summit$text\ntoks = tokens(sum_twt)\nsumtwtdfm &lt;- dfm(toks)\n\nNow, sumtwtdfm should be a document-feature matrix that can be used for further analysis, such as sentiment analysis, word frequency analysis, or topic modeling. We will be performing LSA in the code below:\n\n# Latent Semantic Analysis\nsum_lsa &lt;- textmodel_lsa(sumtwtdfm)\nsummary(sum_lsa)\n\n                Length    Class     Mode   \nsk                     10 -none-    numeric\ndocs               145200 -none-    numeric\nfeatures           160960 -none-    numeric\nmatrix_low_rank 233713920 -none-    numeric\ndata            233713920 dgCMatrix S4     \n\n## Tokenizing the tweet text again, this time removing punctuation, and then creating a new document-feature matrix\ntweet_dfm &lt;- tokens(sum_twt, remove_punct = TRUE) %&gt;%\n  dfm()\nhead(tweet_dfm)\n\nDocument-feature matrix of: 6 documents, 16,029 features (99.89% sparse) and 0 docvars.\n       features\ndocs    breaking news us president biden amp communist china leader xi\n  text1        1    1  1         1     1   1         1     2      1  1\n  text2        0    0  0         0     0   0         0     0      0  0\n  text3        0    0  0         0     1   0         0     0      0  1\n  text4        0    0  0         1     1   0         0     0      0  1\n  text5        0    0  0         0     1   0         0     0      0  1\n  text6        0    0  0         0     1   0         0     0      0  1\n[ reached max_nfeat ... 16,019 more features ]\n\n## Extracting and analyzing hashtags\ntag_dfm &lt;- dfm_select(tweet_dfm, pattern = \"#*\")\ntoptag &lt;- names(topfeatures(tag_dfm, 50))\nhead(toptag, 10)\n\n [1] \"#china\"          \"#biden\"          \"#xijinping\"      \"#joebiden\"      \n [5] \"#america\"        \"#americans\"      \"#coronavirus\"    \"#fentanyl\"      \n [9] \"#xi\"             \"#uyghurgenocide\"\n\n\nSo, the top 10 hashtags used in the tweets are:\n\"#china\"          \"#biden\"          \"#xijinping\"      \"#joebiden\"       \"#america\" \"#americans\"      \"#coronavirus\"    \"#fentanyl\"       \"#xi\"        \"#uyghurgenocide\"\nNow, we are using the quanteda.textplots package to visualize a network plot of co-occurrence relationships among hashtags in the tweet data. The parameters like min_freq, edge_alpha, and edge_size control the appearance of the network plot.\n\n## Visualizing a network plot of relationships among hashtags in tweets\nlibrary(\"quanteda.textplots\")\ntag_fcm &lt;- fcm(tag_dfm)\nhead(tag_fcm)\n\nFeature co-occurrence matrix of: 6 by 685 features.\n               features\nfeatures        #breaking #breakingnews #biden #china #usa #pray4america\n  #breaking             0             4      4      5    5             0\n  #breakingnews         0             0      4      5    4             0\n  #biden                0             0      0    415   44             0\n  #china                0             0      0      8   76             0\n  #usa                  0             0      0      0    6             0\n  #pray4america         0             0      0      0    0             0\n               features\nfeatures        #joebiden #xijinping #america #americans\n  #breaking             0          0        0          0\n  #breakingnews         0          0        0          0\n  #biden              299        366      301        295\n  #china              339        433      308        295\n  #usa                 12         14        0          0\n  #pray4america         0          0        0          0\n[ reached max_nfeat ... 675 more features ]\n\ntopgat_fcm &lt;- fcm_select(tag_fcm, pattern = toptag)\ntextplot_network(topgat_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 5)\n\n\n\n\nThe above plot shows that the most important hashtags that frequently occur together are coronavirus, fentanyl, americans, and the two heads of state who attended the summit. Some hashtags are forming central nodes connecting different clusters, for example, hashtag biden is associated with hashtags of human rights, Uyghurs and Tibetians, whereas hashtag China is associated with hashtags discussing relationships between US, China and Taiwan.\nNext. we are working with user mentions (Twitter handles) in the tweet data. First we select terms (features) from tweet_dfm that match the pattern “@*” (user mentions), then identifiy the top 50 user mentions using the topfeatures function and store their names in the topuser object. The head function is used to display the first 20 user mentions. Then we are creating a feature co-occurence matrix based on document of user mentions, and displaying the first 20 rows. Then the code selects only the co-occurrence relationships of the top user mentions identified earlier from the entire co-occurrence matrix and uses textplot_network to create a network plot of the co-occurrence relationships among the top user mentions.\n\nuser_dfm &lt;- dfm_select(tweet_dfm, pattern = \"@*\")\ntopuser &lt;- names(topfeatures(user_dfm, 50))\nhead(topuser, 20)\n\n [1] \"@potus\"           \"@joebiden\"        \"@politico\"        \"@eneskanter\"     \n [5] \"@jendeben\"        \"@nwadhams\"        \"@nba\"             \"@washwizards\"    \n [9] \"@pelicansnba\"     \"@capitalonearena\" \"@kevinliptakcnn\"  \"@foxbusiness\"    \n[13] \"@morningsmaria\"   \"@scmpnews\"        \"@uyghur_american\" \"@nytimes\"        \n[17] \"@petermartin_pcm\" \"@nahaltoosi\"      \"@phelimkine\"      \"@kaylatausche\"   \n\nuser_fcm &lt;- fcm(user_dfm)\nhead(user_fcm, 20)\n\nFeature co-occurrence matrix of: 20 by 741 features.\n                 features\nfeatures          @youtube @bfmtv @cnn @lauhaim @barackobama @joebiden\n  @youtube               0      0    0        0            0         0\n  @bfmtv                 0      0    1        1            1         1\n  @cnn                   0      0    0        1            1         1\n  @lauhaim               0      0    0        0            1         1\n  @barackobama           0      0    0        0            0         1\n  @joebiden              0      0    0        0            0         3\n  @kamalaharris          0      0    0        0            0         0\n  @hillaryclinton        0      0    0        0            0         0\n  @billclinton           0      0    0        0            0         0\n  @cbsnews               0      0    0        0            0         0\n                 features\nfeatures          @kamalaharris @hillaryclinton @billclinton @cbsnews\n  @youtube                    0               0            0        0\n  @bfmtv                      1               1            1        1\n  @cnn                        1               1            1        1\n  @lauhaim                    1               1            1        1\n  @barackobama                1               1            1        1\n  @joebiden                   1               1            1        1\n  @kamalaharris               0               1            1        1\n  @hillaryclinton             0               0            1        1\n  @billclinton                0               0            0        1\n  @cbsnews                    0               0            0        0\n[ reached max_feat ... 10 more features, reached max_nfeat ... 731 more features ]\n\nuser_fcm &lt;- fcm_select(user_fcm, pattern = topuser)\ntextplot_network(user_fcm, min_freq = 20, edge_color = \"firebrick\", edge_alpha = 0.8, edge_size = 5)\n\n\n\n\nAs the plot shows, the bulk of the Tweet discussion is surrounding the users @capitalonearena, @nba, @eneskanter, @pelicansnba, and @washwizards. Thus, network analysis of users in a Twitter discussion canreveal valuable insights into the structure, dynamics, and patterns of interactions among clusters or communities of users who frequently interact with each other. In other words, it helps identify influential users or hubs who have a large number of connections."
  }
]